---
title: "IBM6250 Group Project - Coffee Vending"
author: "Group 1"
format:
  html:
    toc: true            
    toc-depth: 3         
    self-contained: true
  revealjs:
    theme: simple             
    slide-level: 2            
    incremental: true         
    self-contained: true
    output-file: IBM6250-Coffee-Vending-revealjs.html
editor: visual
bibliography: refrences.bib
---

| Order | Task / Deliverable                           | Owner   | Details |
|------:|----------------------------------------------|---------|---------|
| **1** | Input / Intro section                        | Jarrod  | Project overview and report framing |
| **2** | Exploratory Data Analysis (EDA)              | Min     | Data cleaning, visualization, descriptive stats |
| **3** | Modeling                                     | Eunice  | Build and validate predictive models |
| **4** | Forecast                                     | Ceren   | Predict overall revenue, drink consumption, and ingredient usage |


## Introduction

Effective inventory control for coffee-vending machines hinges on anticipating weekly ingredient consumption while avoiding costly spoilage. We forecast demand using historical sales from two machines, delivering eight-week projections that guide stock levels and reorder cadence.

### This report:

1.  Imports & cleans transaction data from two coffee-vending machines.\
2.  Explores key demand drivers.\
3.  Models weekly sales with Seasonal ARIMA (plus Prophet as a benchmark).\
4.  Delivers eight-week forecasts and stocking recommendations.

## Data Input and Combining

Kaggle data is from two vending machines. Below we will import the two datasets and combine them.

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(tidyverse)
library(lubridate)
library(timetk)
library(tsibble)
library(DT)
library(plotly)
library(tidyr)
library(dplyr)
library(fpp3)
library(scales)
library(fabletools)
library(fable)
library(feasts)
library(skimr)
library(forecast)
library(tseries)
library(gt)
library(plotly)
```

### Raw Transaction Data

Transaction data was taken from the following Kaggle link:

<https://www.kaggle.com/datasets/ihelon/coffee-sales>

```{r load-data, warning=FALSE}
machine1 <- read_csv("data/index_1.csv") %>% 
  mutate(machine_id = "machine1")

sales <- machine1 |>
  mutate(date = as_date(date),
         datetime = as_datetime(datetime),
         coffee_name=toupper(coffee_name))


skim(sales)

unique(machine1$coffee_name)
```

### Products and Ingredients

In the dataset, only product names are given. In order to more accurately predict what ingredients are needed and when, we must decompose the product into its ingredients. See below for the assumptions made for each of the `r length(unique(sales$coffee_name))` unique products.

#### Unique Products

```{r}
unique(machine1$coffee_name)%>%sort()
```

#### Ingredients

```{r recipie list}


recipes <- tribble(
  ~coffee_name,            ~coffeeG, ~milkML, ~chocolateG, ~caramelML, ~sugarG, ~vanillaML,
  "AMERICANO",                18,       0,          0,          0,        0,        0,
  "AMERICANO WITH MILK",      18,      60,          0,          0,        0,        0,
  "CAPPUCCINO",               18,      60,          0,          0,        0,        0,
  "COCOA",                     0,     240,         22,          0,       15,        0,
  "CORTADO",                  18,      60,          0,          0,        0,        0,
  "ESPRESSO",                 18,       0,          0,          0,        0,        0,
  "HOT CHOCOLATE",             0,     240,         30,          0,       20,        0,
  "LATTE",                    18,     240,          0,          0,        0,       10
)

```

##### Ingredient Logic
| Drink | Ingredient-logic rationale |
|-------|---------------------------|
| **Espresso** | Straight double shot: 18 g ground coffee, no additives [@scaEspresso]. |
| **Americano** | Same 18 g espresso diluted with ≈ 4 × its volume of hot water; nothing else required [@scaEspresso]. |
| **Americano&nbsp;with&nbsp;Milk** | Americano softened with ≈ 60 ml steamed milk – enough to mellow bitterness without turning it into a latte [@whiteAmericano]. |
| **Cappuccino** | Classic 1 : 1 : 1 build – espresso, ≈ 60 ml steamed milk, equal micro-foam – fills a 150–180 ml cup [@spruceCappuccino]. |
| **Cortado** | Spanish “cut” drink: equal parts double espresso and ≈ 60 ml steamed milk [@foodwineCortado]. |
| **Latte** | U.S. latte stretches the shot with ≈ 240 ml milk (1 : 4–5 ratio); vanilla version adds 10 ml syrup (≈ 2 pumps) [@coffeeBrosLatte; @toraniPump]. |
| **Cocoa** | Non-coffee mix: 240 ml milk + 22 g cocoa powder + 15 g sugar – standard stovetop proportions [@hersheyCocoa]. |
| **Hot Chocolate** | Richer café blend: same milk but 30 g cocoa and 20 g sugar for modern sweetness level [@hersheyCocoa]. |



### Combining Transaction Data and Recipies

Below we will join the two tables on the coffee name, which will add ingredients to all rows in the transaction data. Explore the data we will use in our analysis below:

```{r join-recipies}
sales_ingredients <- sales |>
  left_join(recipes, by = "coffee_name") |>
  replace_na(list(
    coffee = 0, milk = 0, chocolate = 0, caramel = 0,
    whiskey = 0, tea = 0, vanilla = 0
  ))
```

```{r diplay data with ingredients}
#| echo: false

datatable(
  sales_ingredients,
  extensions = 'Buttons',             
  options = list(
    dom     = 'Bfrtip',                # place Buttons at the top
    buttons = c('copy', 'csv'),
    pageLength = 5
  )
)
```

### Converting to Weekly Series

We aggregate to a weekly time series because the business decisions we are informing, like re-ordering coffee, milk, chocolate, etc, are made on a weekly cadence. Collapsing daily transactions into weeks smooths out erratic, day-to-day swings leaving a cleaner signal that aligns directly with the quantity we must predict.

We will also convert to a time series type object and verify it has no gaps in the series. If we see FALSE from .gaps, then we have no gaps.

```{r weekly data}

weekly_sales <- sales_ingredients |>
  mutate(week = lubridate::floor_date(date, unit = "week")) |>
  group_by(week) |>
  summarise(across(coffeeG:vanillaML, sum, na.rm = TRUE),
            sales_n = n()) |>
  ungroup()

weekly_sales <- weekly_sales|>
  as_tsibble(index = week)

has_gaps(weekly_sales)
```

### Final Data for use in Analysis

```{r diplay weekly data}
#| echo: false

datatable(
  weekly_sales,
  extensions = 'Buttons',             
  options = list(
    dom     = 'Bfrtip',                # place Buttons at the top
    buttons = c('copy', 'csv'),
    pageLength = 5
  )
)
```

```{r interactive-weekly-chart}
#| fig-cap: "Weekly ingredient demand vs. cups sold"
#| echo: false



# 1. Reshape to long format: one row per week-metric pair -------------
weekly_long <- weekly_sales |>
  pivot_longer(
    cols      = coffeeG:sales_n,   # everything after the 'week' column
    names_to  = "metric",
    values_to = "value"
  )

# 2. Build the interactive plot --------------------------------------
plot_ly(
  data  = weekly_long,
  x     = ~week,
  y     = ~value,
  color = ~metric,
  type  = "scatter",
  mode  = "lines+markers",
  hovertemplate = paste(
    "<b>%{x|%Y-%m-%d}</b><br>",
    "%{text}: %{y}<extra></extra>"
  ),
  text = ~metric
) %>% 
  layout(
    hovermode = "x unified",
    legend    = list(title = list(text = "Metric")),
    yaxis     = list(title = "Units"),
    xaxis     = list(title = "Week")
  )

```

## Exploratory Data Analysis(EDA)

```{r}

#weekly total sales per machine
weekly_sales_per_machine <- sales_ingredients |>
  mutate(week = lubridate::floor_date(date, unit = "week")) |>
  group_by(machine_id, week) |>
  summarise(across(coffeeG:vanillaML, sum, na.rm = TRUE),
            sales_n = n(),
            .groups = "drop") |>
  as_tsibble(index = week, key = machine_id)

weekly_sales_per_machine %>% 
  autoplot(sales_n)+
  labs(title = "Weekly Total Sales per Machine",
       y = "Number of Sales",
       x = "Week") +
  guides(color = guide_legend(title = "Machine ID")) +
  theme_minimal()
```

```{r}


#Decomposition of Weekly Ingredient Usage
decomposed <- weekly_long %>%
  filter(metric != "sales_n") %>%
  model(STL(value)) %>%
  components()


autoplot(decomposed) +
  facet_wrap(~metric, scales = "free_y") +
  labs(title = "Decomposition of Weekly Ingredient Usage",
       x = "Week",
       y = NULL) +
  theme_minimal()+
  theme(legend.position = "none")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
#checking Autocorrelation of each ingredients
weekly_long %>%                       # your original data-frame  
  filter(metric != "sales_n") %>%     # keep the same exclusion
  group_by(metric) %>%                # work metric-by-metric
  group_walk(~{
    v <- .x$value
    v <- v[is.finite(v)]             # drop NA / Inf

    if (length(v) < 2L || var(v) == 0) {
      message("Skipping ", .y$metric, ": not enough finite variation.")
    } else {
      acf(v,
          main = paste("ACF for", .y$metric),
          na.action = na.pass)        # keeps plotting even if a few NA remain
    }
  })

```

```{r}
#Differencing each ingredients
weekly_diff <- weekly_long %>%
  filter(metric != "sales_n") %>%
  group_by(metric) %>%  
  arrange(week) %>%  
  mutate(value_diff = difference(value)) %>%  
  filter(!is.na(value_diff)) %>%  
  ungroup()



#ACF for Differenced Ingredients
weekly_diff %>%                          # your differenced data frame
  group_by(metric) %>%                   # handle one metric at a time
  group_walk(~{
    v <- .x$value_diff                   # pull the differenced vector

    # 1. keep only finite, non-missing observations
    v <- v[is.finite(v)]

    # 2. check that we still have at least two values AND some variance
    if (length(v) < 2L || var(v) == 0) {
      message("Skipping ", .y$metric,
              ": not enough finite variation after differencing.")
    } else {
      acf(v,
          lag.max = 30,
          main = paste("ACF for Differenced", .y$metric),
          na.action = na.pass)           # ignore any intermittent NA gaps
    }
  })

```

## Modeling Process

## Model Evaluation and Diagnostics

## Generate Forecasts

## Executive Summary with Actionable Recommendations

## Appendix

#### Machine 1

```{r m1interactive-weekly-chart}
#| fig-cap: "Machine 1 Weekly ingredient demand vs. cups sold"
#| echo: false

weekly_sales_machine1 <- sales_ingredients |>
  filter(machine_id=="machine1") |>
  mutate(week = lubridate::floor_date(date, unit = "week")) |>
  group_by(week) |>
  summarise(across(coffeeG:vanillaML, sum, na.rm = TRUE),
            sales_n = n()) |>
  ungroup()

weekly_sales_machine1 <- weekly_sales_machine1|>
  as_tsibble(index = week)

# 1. Reshape to long format: one row per week-metric pair -------------
weekly_long_machine1 <- weekly_sales_machine1%>%filter() |>
  pivot_longer(
    cols      = coffeeG:sales_n,   # everything after the 'week' column
    names_to  = "metric",
    values_to = "value"
  )

# 2. Build the interactive plot --------------------------------------
plot_ly(
  data  = weekly_long_machine1,
  x     = ~week,
  y     = ~value,
  color = ~metric,
  type  = "scatter",
  mode  = "lines+markers",
  hovertemplate = paste(
    "<b>%{x|%Y-%m-%d}</b><br>",
    "%{text}: %{y}<extra></extra>"
  ),
  text = ~metric
) %>% 
  layout(
    hovermode = "x unified",
    legend    = list(title = list(text = "Metric")),
    yaxis     = list(title = "Units"),
    xaxis     = list(title = "Week")
  )

```

```{}
```
